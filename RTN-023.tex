\documentclass[DM,authoryear,toc]{lsstdoc}
% lsstdoc documentation: https://lsst-texmf.lsst.io/lsstdoc.html
\input{meta}

% Package imports go here.

% Local commands go here.

%If you want glossaries
%\input{aglossary.tex}
%\makeglossaries

\title{Campaign Tooling -- tools for generating, monitoring and tracking data processing campaigns}

% Optional subtitle
% \setDocSubtitle{A subtitle}

\author{%
Brian Yanny, Colin Slater, Sergey Padolski, Kian-Tat Lim, Yusra AlSayyad, Hsin-Fang Chiang, Huan Lin
}

\setDocRef{RTN-023}
\setDocUpstreamLocation{\url{https://github.com/lsst/rtn-023}}

\date{\vcsDate}

% Optional: name of the document's curator
% \setDocCurator{The Curator of this Document}

\setDocAbstract{%
Tools for working with the Butler to query for status of data collections to generate processing campaigns, for working with BPS, PanDA,  Condor or other workflow systems to monitor campaigns; tools to track success and failure of campaigns, past and present. Requirements described.
}

% Change history defined here.
% Order: oldest first.
% Fields: VERSION, DATE, DESCRIPTION, OWNER NAME.
% See LPM-51 for version number policy.
\setDocChangeRecord{%
  \addtohist{1}{2021-08-09}{Initial release for feedback.}{Brian Yanny}
}


\begin{document}

% Create the title page.
\maketitle
% Frequently for a technote we do not want a title page  uncomment this to remove the title page and changelog.
% use \mkshorttitle to remove the extra pages

% ADD CONTENT HERE
% You can also use the \input command to include several content files.

\section{Introduction and Definitions}

Data Processing for Rubin occurs at several levels,
from the lowest level 'quantum' of processing, to the 'task' level for running
the same routine on many quanta, to a 'run' which is a ordered set of 'task's
where the order and tasks are nodes in a 'directed acyclic graph' (DAG).

This document draws heavily from \cite{DMTN-181} which defines campaigns, and 
the scope and role of a campaign management system for data processing.
Some more related definitions are described in \cite{DMTN-137}.

A high-level term to describe processing is 'campaign'.

A 'campaign' is defined as one or more related sets of runs, to carry out a
specific data high-level processing goal.  

Examples of campaigns are:

\begin{itemize}

\item (re)process the DC2 dataset, starting with the raw visit exposures,
 into tracts.  

\item Process last night's data from the mountain top (some 1000-2000 visits 
typically) through single frame processing pipeline steps.

\item Perform FGCM calibration on all consolidated visits within a given
date range.

\item Generate coadds from all 'good' consolidated visits in a given observing
date range that overlap a given set of tracts, use a specific FGCM calibration.

\item Perform Difference Imaging on a specific set of visits, includes gathering
and generation of comparison templates from visits/coadds of date range.

\end{itemize} 

Note: The generic term 'job' is often used to describe processing at 
the 'quantum', 'task' or 'run' level.

It is necessary, when managing large, complex sets of runs in campaigns, 
such as for nightly or annual Data Release Processing (DRP) or for 
Prompt/Alert systems, to:
\begin{itemize}

\item  generate related sets of a campaign's runs in a 
systematic fashion

\item track all runs and their associated control parameters and settings, 
within campaigns when submitted

\item monitor run status for all runs in a campaign, and all currently
running or recently ended campaigns

\item review stdout and stderr and other log files for individual tasks 
within runs, especially when a task or quanta within a task fail.

\end {itemize}

Campaign monitoring tools are also potentially closely connected to
System Performance metrics such as described in \cite{DMTN-173}, for instance'Faro' checks on pipeline outputs \cite{DMTR-311}.

The purpose of this document is to describe requirements on a campaign
tracking and monitoring system that can begin by assisting in DP0 
\cite{RTN-001} production and grow to assisting in DRP and other 
data processing for Rubin.

\section{Campaign Generation and Tracking Requirements}

\begin{figure}
\includegraphics[width=\textwidth]{CampaignTooling.jpg}]
\caption{Diagram of Campaign generation (left) and monitoring system (right).
Credit: C. Slater}
\label{CampaignTooling}
\end{figure}

To generate processing campaigns and track them these features are desired,
as shown in Figure \ref{CampaignTooling}:

	\begin{enumerate}

	\item Ability to access up-to-date source of inputs to-be-processed
		into outputs and their current state 
		(already processed, processed and flagged bad, 
		in processing)

	\begin{itemize}
	
	\item May wish to avoid constantly hitting the butler for
	its contents -- so may wish to have another active source
	of inputs and status of processing -- i.e. another database
	which would only occasionally check with the butler to sync up


	\end{itemize}

	\item Ability to generate BPS submit yaml configs 
	(or other top level workflow submit scripts) which request processing
	of subset of inputs needing to be processed, pipelines to be run,
	outputs described.  This could, for instance, be achieved by having
	a set of template yaml configs -- one for each procesing step
	(Single Frame, Consolidate Visit, Photo calib FGCM, 
	astrometric calib skyMap, Coadd/forced Photometry, Difference
	Imaging Analysis) where a 'sed' script or similiar would fill in
	a visit list, a src catalog list, a tract list, etc

	\begin{itemize}

	\item Should be able to 'split' a large set of inputs into
	serveral managable sized pieces.  For instance, DP0.2 consists of
	about 40,000 visits, and a typical Rubin night of observing will
	take 1,000-2,000 science exposures.   So a managable unit for
	single visit processing might be of size 1000 exposures/visits and
	a DP0.2 processing would split the 40,000 visits into 1000 visit
	groups.  Consideration should be given to how long (wallclock time)
	a single campaign 'unit' may be run and to not generally exceed 
	24 hours to a few days to avoid interruptions from downtimes.
	Also depends on ability of system to recover from interruptions and
	continue without needing to restart the whole campaign from
	the beginning.
	

	\item  Should allow(?) 'last minute editing' of configs before
	submission to tweak something, change a pipeline version or
	visit input list

	\item each campaign should be identified by some unique id string, 
	perhaps same as workflow manager identifying string?

	\end{itemize}

	\item Ability to store and manage a set of BPS submit scripts 
	(or equivalent).
	For instance, as each BPS submit yaml is performed, the yaml
	script contents could be saved in a database with metadata such
	as time of submission, location of physical processing
	(SLAC, IN2P3, UK, Cloud, user-laptop), current state (non started,
	running, finished-no-errors, finished-with-errors)


	\item Ability to quickly access and view logs of each 
	campaign's individual tasks/jobs to determine errors

	\item Ability to view overall state of running and recently
	completed campaigns, without being overwhelmed by long-ago completed
	or abandoned campaigns.

	\item As runs may be distributed 'world-wide' for execution,
	local sites should be able to view only those jobs running at
	their local site on local resources or get a wider 'world-wide' view.

	\item Ability to straight-forwardly dump dataids (or filenames
	with paths) and metadata (exposureid, filter) of inputs or outputs
	from some specific job or task of the processing campaign to
	track down issues and errors.  This ability may be present already
	but some 'helper' scripts could simplify the process.

	\item Ability to watch, in realtime, as jobs or tasks within
	a campaign or set of campaigns are running.  There is a tool	
	called 'bps report' which does this at some level, showing how
	many individual inputs in a campaign set have been processed
	so far.

	\end {enumerate}

\section{BPS and the PanDA system}

For DP0 \cite{RTN-013}, Rubin is using 
the \cite{PanDA} workload data.  The Batch Production 
Service (BPS) \cite{LDM-636} is used as an interface to PanDA.  

BPS:
\begin{enumerate}

\item Accepts a yaml formatted keyword-value set of parameters which
defines a single run. (see example run specification yaml below).

\item After 'bps submit <run-spec.yaml>', queries the butler to generate
a 'quantum graph', a DAG which defines the ordered set of 
pipeline/pipetask which are to be run on which specific set of input 
collections (or subsets of a collection which may be picked out via SQL
type where clauses)

\item Generates an ordered list of tasks, each of which may consist
of hundreds of quanta, and submits this list submitted to a 
workload/workflow system (such as HT-Condor or PanDA).

\item Monitors runs which have been submitted  
	(in the HT-Condor case see Fig. \ref{bpsreport}, currently, could be expanded to PanDA) 

\begin{figure}
\includegraphics[width=\textwidth]{bpsreport.jpg}]
\caption{Sample BPS report display showing number of quanta for each task within a give run.
Credit: M. Gower}
\label{bpsreport}
\end{figure}


\end{enumerate}

PanDA:

\begin{enumerate}

\item Accepts an ordered list of tasks (from BPS), which specify what
pipelines to run on which inputs.

\item Distributes tasks to worker nodes, which may be widely distributed.

\item Monitors tasks and associated quanta. See Figs. \ref{pandadash0},\ref{tasks1},\ref{tasks4},\ref{tasks5}

\begin{figure}
\includegraphics[width=\textwidth]{pandadash0.jpg}]
\caption{Top level PanDA monitoring dashboard of running and recent jobs
Credit: S. Padolski}
\label{pandadash0}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{tasks1.jpg}]
\caption{Next level task list for first job in list
Credit: H. Lin}
\label{tasks1}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{tasks4.jpg}]
\caption{Next level status for isr task (2nd from bottom in prev list).
Credit: H. Lin}
\label{tasks4}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{tasks5.jpg}]
\caption{Next level status for some of the 603 quanta in this task.
Credit: H. Lin}
\label{tasks5}
\end{figure}

\item Allows viewing of log files for individual tasks. See Fig \ref{log0},\ref{log1}

\begin{figure}
\includegraphics[width=\textwidth]{log0.jpg}]
\caption{Getting to the stdout/stderr log for this quantum. 
Credit: H. Lin}
\label{log0}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{log1.jpg}]
\caption{A look at stdout/stderr -- if there was an error, one can view it here to help diagnose an issue.
Credit: H. Lin}
\label{log1}
\end{figure}

\item Allows for retries with more memory for individual tasks that fail.

\item More!

\end{enumerate}

Please see \cite{DMTN-168} for more details on running with BPS and PanDA.

\section{Sample BPS 'run' specification yaml}

A 'run' is defined as a run of a BPS-run-spec.yaml through 'bps submit
BPS-run-spec.yaml'.

This run-spec yaml file defines:

\begin{enumerate}

\item An ordered list of pipetasks to be run: 
\begin{verbatim}
pipelineYaml: 
"$OBS_LSST_DIR/pipelines/imsim/DRP.yaml#singleFrame,multiVisit"
\end{verbatim}
These pipetasks may consist of several steps, and these steps may be defined
in 'includes' in the DRP.yaml, for instance.

\item A target cluster to run on, and resource requests such as 
maximum memory (which can be different for different pipelines), number of cpus,
and maximum wallclock time, and re-try attempt limits.

\item A butler configuration yaml

\item an input collection to be extracted from the butler mentioned in
the butler configuration along with a dataQuery which can restict the inputs
to a subset of the full input collection based on 
an SQL string (i.e. just do a few visits or a single tract
or a single band)

\item various other configuration information including location of 
buckets, output path specs, etc

\end{enumerate}

Here is an Example look at a run specification yaml script. Each run in a
campaign is driven by a single yaml-style configuation script which is 
launched by 'bps submit':

\begin{verbatim}
...

pipelineYaml: "$OBS_LSST_DIR/pipelines/imsim/DRP.yaml#singleFrame,multiVisit"
project: dev
campaign: "configuration_example"
submitPath: ${HOME}/submit/{outCollection}

...
computing_cloud: LSST
maxwalltime: 90000
requestMemory: 2000
maxattempt: 1

...

payload:
  runInit: true
  payloadName: singleFrame_multiVisit
  butlerConfig: s3://butler-us-central1-panda-dev/dc2/butler.yaml
  inCollection: 2.2i/defaults/DP0.2
  output: "u/${USER}/panda_test_dp0_tract_3828_patch_18_w26_iband"
  outCollection: "{output}/{timestamp}"
  dataQuery: "instrument='LSSTCam-imSim' and skymap='DC2' and (tract=3828 and patch=18 and band='i')"
  sw_image: "spodolsky/centos:7-stack-lsst_distrib-w_2021_26"
  bucket: "s3://butler-us-central1-panda-dev/hsc"
  s3_endpoint_url: "https://storage.googleapis.com"


...
requestCpus: 1
...
templateDataId: "{tract}_{patch}_{band}_{visit}_{exposure}_{detector}"


\end{verbatim}


\section{Next steps in Campaign Tooling Development}

A visual description of how a typical DRP campaign may be broken into
related runs is shown in Fig. \ref{WorkflowExampleRunSet}.

\begin{figure}
\includegraphics[width=\textwidth]{analdrp.png}]
\caption{Diagram of Data Release Processing data flow.
Credit: Y. AlSayyad, H. Chiang}
\label{WorkflowExampleRunSet}
\end{figure}

To begin, Campaign Tooling plans develop tools to assist with
campaign generation and tracking for the specific DP0.2 milestone 
of reprocessing the DC2 DESC data set in the iDF using BPS
and PanDA \cite{RTN-013}.  

Figure \ref{WorkflowExampleRunSet} can serve as an initial guide to
what is needed for generation of the DP0.2 campaign (left side of Figure 
\ref{CampaignTooling}): One run-spec.yaml file for each box in the Figure.  

In order to implement the right side of Figure \ref{CampaignTooling}, one may
need to setup a database to hold and visualize the run-spec definitions, 
as well as add interface items to BPS and PanDA to get feedback on running
or submitted jobs.  Some existing tools for generating and tracking 
workflows are \cite{airflow} and \cite{POMS}.

\appendix
% Include all the relevant bib files.
% https://lsst-texmf.lsst.io/lsstdoc.html#bibliographies
\section{References} \label{sec:bib}
\renewcommand{\refname}{} % Suppress default Bibliography section
\bibliography{local,lsst,lsst-dm,refs_ads,refs,books}

% Make sure lsst-texmf/bin/generateAcronyms.py is in your path
\section{Acronyms} \label{sec:acronyms}
\input{acronyms.tex}
% If you want glossary uncomment below -- comment out the two lines above
%\printglossaries




\end{document}
