\documentclass[DM,authoryear,toc]{lsstdoc}
\usepackage{tikz}
% lsstdoc documentation: https://lsst-texmf.lsst.io/lsstdoc.html
\input{meta}

% Package imports go here.

% Local commands go here.

%If you want glossaries
%\input{aglossary.tex}
%\makeglossaries

\title{Campaign Tooling -- tools for generating, monitoring and tracking data processing campaigns}

% Optional subtitle
% \setDocSubtitle{A subtitle}

\author{%
Brian Yanny, Colin Slater, Sergey Padolski, Kian-Tat Lim, Michelle Gower, Yusra AlSayyad, Hsin-Fang Chiang, Huan Lin
}

\setDocRef{RTN-023}
\setDocUpstreamLocation{\url{https://github.com/lsst/rtn-023}}

\date{\vcsDate}

% Optional: name of the document's curator
% \setDocCurator{The Curator of this Document}

\setDocAbstract{%
Tools for working with the Butler to query for status of data collections to generate processing campaigns, for working with BPS, PanDA,  Condor or other workflow systems to monitor campaigns; tools to track success and failure of campaigns, past and present. Requirements described.
}

% Change history defined here.
% Order: oldest first.
% Fields: VERSION, DATE, DESCRIPTION, OWNER NAME.
% See LPM-51 for version number policy.
\setDocChangeRecord{%
  \addtohist{1}{2021-08-09}{Initial release for feedback.}{Brian Yanny}
  \addtohist{2}{2021-08-17}{Addressing comments in pull/2.}{Brian Yanny}
}


\begin{document}

% Create the title page.
\maketitle
% Frequently for a technote we do not want a title page  uncomment this to remove the title page and changelog.
% use \mkshorttitle to remove the extra pages

% ADD CONTENT HERE
% You can also use the \input command to include several content files.

\section{Introduction and Definitions}

Data Processing for Rubin occurs at several levels,
from the lowest level 'quantum' of processing, to the 'task' level 
for running the same routine on many quanta, to a 'run' which is a 
ordered set of tasks where the order and tasks are nodes 
in a 'directed acyclic graph' (DAG).

This document draws heavily from \cite{DMTN-181} which defines campaigns, and 
the scope and role of a campaign management system for data processing.
Some more related definitions are described in \cite{DMTN-137}.
The difference between visit and exposure ids is explained in \cite{iddefs}.

A high-level term to describe processing is 'campaign'.

A 'campaign' is defined as one or more related sets of runs, to carry out a
specific data high-level processing goal.  

Examples of campaigns are:

\begin{itemize}

\item (re)process the DC2 dataset, starting with the raw calibration and 
visit exposures, into tracts.  

\item Create flat-field, dark, bias calibrations from raw flat-field,
dark, bias exposures.  Build master calibrations.

\item Process last night's data from the mountain top (some 1000-2000 visits 
typically) through the single frame processing steps.

\item Perform FGCM calibration on all consolidated visits within a given
date range.

\item Generate coadds from a set of consolidated visits in a given observing
date range that overlap a given set of tracts; use a specific FGCM calibration.
The set of consolidated visits may be chosen based on a set of science quality 
metrics for each visit, which allows cuts on parameters such as 
seeing $<$ seeing-threshold, sky brightness $<$ sky-threshold, object count 
$>$ count-threshold, etc.

\item Perform Difference Imaging Analysis on a specific set of visits, 
which may include generation of comparison templates from visits/coadds 
of a given date range.

\item Make a data release (i.e. DR1).  A very high level campaign!

\end{itemize} 

It is necessary, when managing large, complex sets of runs in campaigns, 
such as for nightly or annual Data Release Processing (DRP) to:

\begin{itemize}

\item  generate parameters for describing related sets of a campaign's 
runs in a systematic fashion.

\item track all runs and their associated control parameters and settings
for a campaign.

\item monitor run status for all runs within a campaign, and all currently
running or recently ended campaigns

\item review stdout and stderr and other log files for individual jobs
within runs, especially when a job or quanta within a job fails.

\end {itemize}

Campaign monitoring tools are also potentially closely connected to
System Performance metrics such as described in \cite{DMTN-173}, 
for instance \texttt{Faro} checks on pipeline outputs \cite{DMTR-311}, \cite{FARO}.

The purpose of this document is to describe requirements on a campaign
tracking and monitoring system that can begin by assisting in DP0 
\cite{RTN-001} production and grow to assisting in DRP and other 
data processing for Rubin.

\section{Campaign Generation and Tracking Requirements}

\begin{figure}
\includegraphics[width=\textwidth]{CampaignTooling.jpg}]
\caption{Diagram of Campaign generation (left) and monitoring system (right).
Credit: C. Slater}
\label{CampaignTooling}
\end{figure}

To generate processing campaigns and track them these features are desired,
as shown in Figure \ref{CampaignTooling}:

	\begin{enumerate}

	\item Ability to generate BPS submit yaml configs 
	(or other top level workflow submit scripts) which request processing
	of subset of inputs needing to be processed, pipelines to be run,
	outputs described.  This could, for instance, be achieved by having
	a set of template yaml configs -- one for each processing step (see step
	list for DRP below) (generate calibs from raw calibs, Single Frame, Consolidate Visit, 
	Photo calib FGCM, astrometric calib skyMap, 
	make coadd for a set of tracts, run photometry on the coadd,
	run forced Photometry on the individual visits of a coadd, Difference
	Imaging Analysis) where a 'sed' script or similiar would fill in
	a visit list, a src catalog list, a tract list, etc.

	\begin{itemize}

	\item Should be able to 'split' a large set of inputs into
	serveral managable sized pieces.  For instance, DP0.2 consists of
	about 40,000 visits, and a typical Rubin night of observing will
	take 1,000-2,000 science exposures.   So a managable unit for
	single visit processing might be of size 1000 exposures/visits and
	a DP0.2 processing would split the 40,000 visits into 1000 visit
	groups.  Consideration should be given to how long (wallclock time)
	a single campaign 'unit' may be run and to not generally exceed 
	24 hours to a few days to avoid interruptions from downtimes.
	Also depends on ability of system to recover from interruptions and
	continue without needing to restart the whole campaign from
	the beginning.
	

	\item  Should allow 'last minute editing' of configs before
	submission to tweak something, change a pipeline version or
	visit input list.  This means that whatever records the campaign
	parameters is able to record the change as well and not get
	out of sync with the by-hand or last minute change.

	\item Each campaign should be identified by some unique id string, 
	perhaps same as workflow manager identifying string?
	There is a concept of a RUN which can serve as at least part of an
	identifying string.

	\end{itemize}

	\item Ability to store and manage a set of BPS submit scripts 
	(or equivalent).

	While one doesn't wish to reproduce the butler registry, there
	is a need to store somewhere campaign or run 'tuples' which
	can be a set of BPS submit file.yaml along with information
	such as time-of-submission and current status.  


	\item Ability to quickly access and view logs of each 
	campaign's individual tasks/jobs to determine errors

	\item Ability to view overall state of running and recently
	completed campaigns, without being overwhelmed by long-ago completed
	or abandoned campaigns.

	\item As centrally (DF) submitted runs may be distributed to 
	multiple sites for execution, managers at local sites should 
	be able to view only those jobs running at their local site on local 
	resources.

	\item Ability to watch, in realtime, as jobs or tasks within
	a campaign or set of campaigns are running.  

	\end {enumerate}

	The JIRA system is a candidate place where these campaign-grouped submit file
	run 'tuples' could be stored.  Each campaign would have a JIRA ticket number
	and could include sub-tickets on each BPS submit run for the campaign.
	Another option would be a separate (outside or inside the Butler) Database table such as 
	that which might hold an exposure catalog.

\section{BPS and the PanDA system}

For DP0 \cite{RTN-013}, Rubin is using 
the \cite{PanDA} workload data.  The Batch Production 
Service (BPS) \cite{LDM-636} is used as an interface to PanDA, with
some BPS use cases described in \cite{LDM-633}.  

BPS:
\begin{enumerate}

\item BPS accepts a yaml formatted keyword-value set of parameters which
defines a single run. (see example run specification yaml below).

\item After 'bps submit run-spec.yaml' is entered on the command line, 
BPS launches a butler query to 
generate a 'quantum graph', a DAG which defines the ordered set of 
pipetasks which are to be run on which specific set of input 
collections (or subsets of a collection which may be picked out via SQL-style
where clauses).  

\item BPS generates for the requested pipeline, an ordered list of jobs, 
each of which may consist of hundreds of quanta, and submits this 
list to a workload system (such as HT-Condor or PanDA).

\item The BPS system, in Execution Butler mode (which is the default mode for processing) creates, 
and after the quantum graph is generated for the run, a small (currently sqlLite) database 
containing all the necessary Butler information (metadata and data storage link URIs) to process 
all steps in the run.  This Execution Butler is constantly updated by the processing the data in question.

\item If data is being processed at a remote site, 
the bps submit step could also help initiate the moving of centrally stored files to a remote processing
data store, using, for instance, Rucio+FTS.

\item BPS monitors runs which have been submitted  (in the HT-Condor case 
see Fig. \ref{bpsreport};  in the PanDA case, there may need to be more
feedback send back, or pointers on how to poll for status).

\item When the processing of all tasks in a run completes, a 'merge' step is run by BPS which syncs the 
small Execution Butler metadata, include info on any new files generated during processing,
back into the main Butler.  

\item If data is being processed at a remote site, 
the merge step could also help initiate the moving of local file back to a central
data store, using, for instance, Rucio+FTS.

\begin{figure}
\includegraphics[width=\textwidth]{bpsreport.jpg}]
\caption{Sample BPS report display showing number of quanta for each task within a give run.
Credit: M. Gower}
\label{bpsreport}
\end{figure}


\end{enumerate}

PanDA:

\begin{enumerate}

\item Accepts an ordered list of tasks (from BPS), which specify what
pipetasks to run on which inputs.

\item Distributes tasks to worker nodes, which may be widely distributed.

\item Monitors tasks and associated quanta. 
See Figs. \ref{pandadash0},\ref{tasks1},\ref{tasks4},\ref{tasks5}

\end{enumerate}

\subsection{Monitoring jobs in PanDA} 

PanDA can accept http:// url queries to access the current status of running and completed jobs.

These URLs may be accessed with a python script.  http:// may be used rather than https:// to avoid
needed to authenticate credentials.

Warning: There is currently a limit of some 500 requests/hour to avoid getting IP blocked.
This can be adjusted if needed.

A term 'json' may be added to the URL to format the output as json text.

Sample syntax for PanDA running/completed job status queries::

\begin{verbatim}
http://panda-doma.cern.ch/jobs/?date_from=21-09-2021&date_to=22-09-2021
\end{verbatim}

There are three levels of monitoring: workflows, tasks and then jobs at the lowest level.

One sees (current) workflows with a query such as:

\begin{verbatim}
http://panda-doma.cern.ch/idds/wfprogress
\end{verbatim}

or in json format:
\begin{verbatim}
http://panda-doma.cern.ch/idds/wfprogress/?json
\end{verbatim}

(this seems to not quite be in standard json format, but look at the raw output to extract tasknames)

This returns json formatted text which includes "run names" r\_name
(one per BPS submit), one can pick out one or more run names of interest and look up the tasks associated
with that run name (note the * construct in the URL):

\begin{verbatim}
http://panda-doma.cern.ch/tasks/?taskname=u_huanlin_panda_test_ci_imsim_d_2021_09_15_w38_20210923t155401z*&json
\end{verbatim}

By default the search looks for jobs within the last 7 days, one may search a range of times/dates
by specifying a date or datetime string:

\begin{verbatim}
https://panda-doma.cern.ch/jobs/?date_from=21-09-2021T10:00&date_to=21-09-2021T11:00
\end{verbatim}

This particular run, which is a submission of a 'step3' (see below) from the
DRP processing list, contains 14 'tasks' (as PanDA calls them).
These have names:

\begin{verbatim}
PREFIX="u_huanlin_panda_test_ci_imsim_d_2021_09_15_w38_20210923t155401z"

reqid: 6237 taskname: ${PREFIX}_deblend_3811
reqid: 6236 taskname: ${PREFIX}_healSparsePropertyMaps_3809
reqid: 6235 taskname: ${PREFIX}_transformObjectTable_3816
reqid: 6234 taskname: ${PREFIX}_mergeDetections_3810
reqid: 6233 taskname: ${PREFIX}_selectGoodSeeingVisits_3804
reqid: 6232 taskname: ${PREFIX}_measure_3812
reqid: 6231 taskname: ${PREFIX}_templateGen_3807
reqid: 6230 taskname: ${PREFIX}_assembleCoadd_3806
reqid: 6229 taskname: ${PREFIX}_makeWarp_3805
reqid: 6228 taskname: ${PREFIX}_pipetaskInit_3803
reqid: 6227 taskname: ${PREFIX}_detection_3808
reqid: 6226 taskname: ${PREFIX}_consolidateObjectTable_3817
reqid: 6225 taskname: ${PREFIX}_forcedPhotCoadd_3814
reqid: 6224 taskname: ${PREFIX}_mergeMeasurements_3813
reqid: 6223 taskname: ${PREFIX}_writeObjectTable_3815

\end{verbatim}

Note that the order here of the reqid's doesn't match the order
of the tasks in the step3 definition, which is in this file:

\begin{verbatim}
$OBS_LSST_DIR/pipelines/imsim/DRP.yaml
\end{verbatim}

This file defines step3 as these tasks in this order:
    step3:
        subset:
\begin{enumerate}
        \item makeWarp
        \item assembleCoadd
        \item detection
        \item mergeDetections
        \item deblend
        \item measure
        \item mergeMeasurements
        \item forcedPhotCoadd
        \item transformObjectTable
        \item writeObjectTable
        \item consolidateObjectTable
        \item healSparsePropertyMaps
        \item selectGoodSeeingVisits
        \item templateGen
\end{enumerate}

Sorting the tasks by the numerical suffix at the end of the task name:

\begin{verbatim}

reqid = task

reqid: 6228 taskname: ${PREFIX}_pipetaskInit_3803
reqid: 6233 taskname: ${PREFIX}_selectGoodSeeingVisits_3804
reqid: 6229 taskname: ${PREFIX}_makeWarp_3805
reqid: 6230 taskname: ${PREFIX}_assembleCoadd_3806
reqid: 6231 taskname: ${PREFIX}_templateGen_3807
reqid: 6227 taskname: ${PREFIX}_detection_3808
reqid: 6236 taskname: ${PREFIX}_healSparsePropertyMaps_3809
reqid: 6234 taskname: ${PREFIX}_mergeDetections_3810
reqid: 6237 taskname: ${PREFIX}_deblend_3811
reqid: 6232 taskname: ${PREFIX}_measure_3812
reqid: 6224 taskname: ${PREFIX}_mergeMeasurements_3813
reqid: 6225 taskname: ${PREFIX}_forcedPhotCoadd_3814
reqid: 6223 taskname: ${PREFIX}_writeObjectTable_3815
reqid: 6235 taskname: ${PREFIX}_transformObjectTable_3816
reqid: 6226 taskname: ${PREFIX}_consolidateObjectTable_3817

One may lookup status information about a specific task (=reqid) by using this syntax:

For status of the deblend task:

https://panda-doma.cern.ch/task/6232/?json

\end{verbatim}

\subsection{More PanDA monitoring}

\begin{enumerate}

\begin{figure}
\includegraphics[width=\textwidth]{pandadash0.jpg}]
\caption{Top level PanDA monitoring dashboard of running and recent jobs
Credit: S. Padolski}
\label{pandadash0}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{tasks1.jpg}]
\caption{Next level task list for first job in list
Credit: H. Lin}
\label{tasks1}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{tasks4.jpg}]
\caption{Next level status for isr task (2nd from bottom in prev list).
Credit: H. Lin}
\label{tasks4}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{tasks5.jpg}]
\caption{Next level status for some of the 603 quanta in this task.
Credit: H. Lin}
\label{tasks5}
\end{figure}

\item Allows browsing of log files from individual tasks. 
See Fig \ref{log0},\ref{log1}

\begin{figure}
\includegraphics[width=\textwidth]{log0.jpg}]
\caption{Getting to the stdout/stderr log for this quantum. 
Credit: H. Lin}
\label{log0}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{log1.jpg}]
\caption{A look at stdout/stderr -- if there was an error, one can view it here to help diagnose an issue.
Credit: H. Lin}
\label{log1}
\end{figure}

\item Allows for retries with more memory for individual tasks that fail.

\item There are graphana, kibana elasticsearch plots that can be setup to monitor overall
PanDA system status and performance.

\end{enumerate}

Please see \cite{DMTN-168} for more details on running with BPS and PanDA.

\section{Sample BPS 'run' specification yaml}

A 'run' is defined as a run of a BPS-run-spec.yaml through 'bps submit
BPS-run-spec.yaml'.

This run-spec yaml file defines:

\begin{enumerate}

\item An existing (sub)pipeline definition to use for the run:
\begin{verbatim}
pipelineYaml: "$OBS_LSST_DIR/pipelines/imsim/DRP.yaml#step3"
\end{verbatim}
These pipetasks may consist of several steps, and these steps may be defined
in 'includes' in the DRP.yaml, for instance.

\item A target cluster to run on, and resource requests such as 
maximum memory (which can be different for different pipelines), number of cpus,
and maximum wallclock time, and re-try attempt limits.

\item The location of the butler respository

\item an input collection to be extracted from the butler mentioned in
the butler configuration along with a dataQuery which can restict the inputs
to a subset of the full input collection based on 
an SQL string (i.e. just do a few visits or a single tract
or a single band)

\item various other configuration information including location of 
buckets, output path specs, etc

\end{enumerate}

Here is an example look at a run specification yaml script. This one 
performs 'step3' pipeline tasks on a input collection of data. 
A full 'campaign' would be a set of perhaps 6 of these files, one
for step1, one for step2, etc.
Various critical name-value parameters are shown. Each run in a
campaign is (currently) driven by a single yaml-style 
configuation script which is launched by 'bps submit':

\begin{verbatim}
#includeConfigs:
#- ${CTRL_BPS_DIR}/python/lsst/ctrl/bps/wms/panda/conf_example/pipelines_check_idf.yaml

pipelineYaml: "$OBS_LSST_DIR/pipelines/imsim/DRP.yaml#step3"

payload:
  payloadName: pipelines_check
  runInit: true
#  output: "u/{operator}/{payload_name}"
  output: "u/{operator}/panda_test_ci_imsim_d_2021_09_15_w38"
  outCollection: "{output}/{timestamp}"
  butlerConfig: s3://butler-us-central1-panda-dev/dc2/butler.yaml
  inCollection: "2.2i/defaults/ci_imsim"
  dataQuery: "instrument='LSSTCam-imSim' and skymap='DC2' and (visit=256383 and detector=26)"
#  dataQuery: "tract = 9615 and patch=30 and detector IN (10..11) and instrument='HSC' and skymap='hsc_rings_v1' and band in ('r')"

  sw_image: "lsstsqre/centos:7-stack-lsst_distrib-w_2021_38"
  fileDistributionEndPoint: "s3://butler-us-central1-panda-dev/hsc/{payload_folder}
/{uniqProcName}/"
  s3_endpoint_url: "https://storage.googleapis.com"
  payload_folder: payload
  runner_command: 'docker run --network host --privileged --env AWS_ACCESS_KEY_ID=$
(</credentials/AWS_ACCESS_KEY_ID) --env AWS_SECRET_ACCESS_KEY=$(</credentials/AWS_S
ECRET_ACCESS_KEY) --env PGPASSWORD=$(</credentials/PGPASSWORD) --env S3_ENDPOINT_UR
L=${S3_ENDPOINT_URL} {sw_image} /bin/bash -c "source /opt/lsst/software/stack/loadL
SST.bash;cd /tmp;ls -a;setup lsst_distrib;pwd;python3 \${CTRL_BPS_DIR}/python/lsst/
ctrl/bps/wms/panda/edgenode/cmd_line_decoder.py _cmd_line_ " >&2;'

#PANDA plugin specific settings:
idds_server: "https://aipanda015.cern.ch:443/idds"
placeholderParams: ['qgraphNodeId', 'qgraphId']

#IDF PanDA specific settings:
computing_cloud: LSST

#SLAC PanDA specific settings:
#computing_cloud: US
#computeSite: DOMA_LSST_SLAC_TEST

operator: huanlin    # defaults to login on submit machine
project: dev
campaign: quick

#....

\end{verbatim}

\section{Implementation Discussion Points}

Here are some details of generating and tracking campaigns that will
require further discussion regarding implemtation details.

	\begin{enumerate}

	\item Ability to access up-to-date source of inputs to-be-processed
		into outputs and their current state 
		(already processed, processed and flagged bad, 
		in processing)

	\begin{itemize}
	
	\item This could be done 'by hand' with butler queries initially
	to determine ranges of visits to process from specific date
	ranges (i.e. last night) or simulated data sets (i.e. DC2).

	\item One will eventually need well defined, efficient ways
	to learn from the butler what is in there to be processed and
	what has already been processed, and if not, why not.
	
	\item Efficiency is a consideration, as one does not wish to
	continually poll the butler or PanDA for the state of all past 
	millions of visits, for example.  This can be done with data to/from
	range terms in the PanDA query.

	\item Helper scripts can assist with dumping dataids (or filenames
	with paths) and other info from the butler related to
	what is needed for campaign generation. This also helps when
	browseing logs and tracking down errors.

	\end{itemize}

	\item Ability to store and manage a set of BPS submit scripts 
	(or equivalent).

	\begin{itemize}

	\item While one doesn't wish to reproduce the butler registry, there
	is a need to store somewhere campaign or run 'tuples' which
	can be a set of BPS submit file.yaml along with information
	such as time-of-submission and current status.

	\item The store of bps submit scripts won't have 'visit level'
	information, only 'ranges' as described in the SQL Butler 
	query (visit-id between 236567 and 237673).

	\item For instance, as each BPS submit file.yaml is performed, the yaml
	script contents or a subset of 'tuples' from the yaml
	could be saved in a relatively lightweight database.  Could
	sqlLite or postgres be used for this?
	In \cite{DMTN-181} it is proposed that there would be a new table
	in the butlerregistry schema that would hold 'DataID sets'.  But
	perhaps an external lightweight database might be an alternative,
	as the butler will still be the ultimate source of metadata
	knowledge about input and output collections.  The external
	database of tuples with BPS submit scripts don't need to
	have exact visit lists so long as the query used to generate
	the input lists are available.

	Components of the tuple would be (input-query, bps-script,
	status,time-of-submission,...). The bps-script file.yaml contains
	such things as the pipelineYaml (list of tasks to run), 
	the sw-image (what software stack is being run).
	The site where the jobs are to be run 
	(SLAC, IN2P3, UK, Cloud, user-laptop).

	\item The 'status' will need to be updated by getting information back
	from PanDA or Condor or whatever workflow system is being used whenever
	there is a change or at some interval.

	\end{itemize}

	\item Ability to watch, in realtime, as jobs or tasks within
	a campaign or set of campaigns are running.  

	Here a web display that reads the lightweight database and
	shows/sorts the different running/recently-completed campaign 
	components would be useful.

	There is a tool	
	called 'bps report' which does this at some level in the condor
	workload system, showing how many individual inputs in a set of
	jobs have been processed so far.

	To work with PanDA there may need to be feedback from the PanDA
	system back to BPS or some other monitoring/tracking system,
	or 'deep links' into the PanDA IDDS could be generated for 
	viewing.

\end{enumerate}

\section{Next steps in Campaign Tooling Development}

A visual description of how a typical DRP campaign may be broken into
related runs is shown in Fig. \ref{WorkflowExampleRunSet}.

\input{makeprocessexamplefigure}
\label{WorkflowExampleRunSet}

%\begin{figure}
%\includegraphics[width=\textwidth]{analdrp.png}]
%\caption{Diagram of Data Release Processing data flow.
%Credit: Y. AlSayyad, H. Chiang}
%\end{figure}

To begin, Campaign Tooling plans to develop tools to assist with
campaign generation and tracking for the specific DP0.2 milestone 
of reprocessing the DC2 DESC data set in the iDF using BPS
and PanDA \cite{RTN-013}.  

Figure \ref{WorkflowExampleRunSet} can serve as an initial guide to
what is needed for generation of the DP0.2 campaign (left side of Figure 
\ref{CampaignTooling}): One run-spec.yaml file for each box in the Figure.  

In order to implement the right side of Figure \ref{CampaignTooling}, one may
need to setup a database to hold and visualize the run-spec definitions, 
as well as add interface items to BPS and PanDA to get feedback on running
or submitted jobs.  Some existing tools for generating and tracking 
workflows are \cite{airflow} and \cite{POMS} as well as the Pegasus Ensemble Manager \cite{PEM}.

\section{DRP.yaml steps}
A recent version of the DRP.yaml file which define steps 1-7 in terms of pipelines/pipetasks units
to be run:

\begin{verbatim}

description: DRP specialized for LSSTImSim
instrument: lsst.obs.lsst.LsstCamImSim
imports:
  # Inherits directly from pipe_tasks to avoid redefining sourceTable subset
  - location: $PIPE_TASKS_DIR/pipelines/DRP.yaml
tasks:
    isr:
        class: lsst.ip.isr.IsrTask
        config:
            connections.newBFKernel: bfk
            doDefect: False
            doBrighterFatter: True
    calibrate:
        class: lsst.pipe.tasks.calibrate.CalibrateTask
        config:
            connections.astromRefCat: 'cal_ref_cat_2_2'
            connections.photoRefCat: 'cal_ref_cat_2_2'
            astromRefObjLoader.ref_dataset_name: 'cal_ref_cat_2_2'
            photoRefObjLoader.ref_dataset_name: 'cal_ref_cat_2_2'
            python: >
                config.astromRefObjLoader.filterMap = {band: 'lsst_%s_smeared' % (band) for band in 'ugrizy'};
                config.photoRefObjLoader.filterMap = {band: 'lsst_%s_smeared' % (band) for band in 'ugrizy'};
    measure:
        class: lsst.pipe.tasks.multiBand.MeasureMergedCoaddSourcesTask
        config:
            connections.refCat: 'cal_ref_cat_2_2'
            match.refObjLoader.ref_dataset_name: 'cal_ref_cat_2_2'
            python: >
                config.match.refObjLoader.filterMap = {band: 'lsst_%s_smeared' % (band) for band in 'ugrizy'};
subsets:
    step1:
        subset:
        - isr
        - characterizeImage
        - calibrate
        - writeSourceTable
        - transformSourceTable
        description: >
            Per-detector tasks that can be run together to start the DRP pipeline.

            These may or may not be run with 'tract' or 'patch' as part of the data ID
            expression. This specific pipeline contains no tasks that require full
            visits. Running with 'tract' (and 'patch') constraints will select
            partial visits that overlap that region.

            In data release processing, operators should stop to address unexpected
            failures before continuing on to step2.
    step2:
        subset:
        - consolidateSourceTable
        - consolidateVisitSummary
        - makeCcdVisitTable
        - makeVisitTable
        description: >
            Per-visit tasks that can be run together, but only after the 'step1'.

            These may or may not be run with 'tract' or 'patch' as part of the data ID
            expression.  Running with 'tract' (and 'patch') constraints will select
            partial visits that overlap that region.
            This specific pipeline contains no tasks that require full visits.

            This subset is considered a workaround for missing middleware and task
            functionality.  It may be removed in the future.
    step3:
        subset:
        - makeWarp
        - assembleCoadd
        - detection
        - mergeDetections
        - deblend
        - measure
        - mergeMeasurements
        - forcedPhotCoadd
        - transformObjectTable
        - writeObjectTable
        - consolidateObjectTable
        - healSparsePropertyMaps
        - selectGoodSeeingVisits
        - templateGen
        description: >
            Tasks that can be run together, but only after the 'step1' and 'step2'
            subsets.

            These should be run with explicit 'tract' constraints essentially all the
            time, because otherwise quanta will be created for jobs with only partial
            visit coverage.

            It is expected that many forcedPhotCcd quanta will "normally" fail when
            running this subset, but this isn't a problem right now because there
            are no tasks downstream of it.  If other tasks regularly fail or we add
            tasks downstream of forcedPhotCcd, these subsets or the tasks will need
            additional changes.

            This subset is considered a workaround for missing middleware and task
            functionality.  It may be removed in the future.
    step4:
        subset:
        - forcedPhotCcd
        - forcedPhotDiffim
        - getTemplate
        - imageDifference
        - transformDiaSourceCat
        description: >
            Tasks that can be run together, but only after the 'step1', 'step2'
            and 'step3' subsets

            These detector-level tasks should not be run with
            'tract' or 'patch' as part of the data ID expression if all
            reference catalogs or diffIm templates that cover these
            detector-level quanta are desired.
    step5:
        subset:
        - drpAssociation
        - drpDiaCalculation
        - forcedPhotCcdOnDiaObjects
        - forcedPhotDiffOnDiaObjects
        description: >
            Tasks that can be run together, but only after the 'step1', 'step2',
            'step3', and 'step4' subsets

            This step includes tract-level aggregation Tasks. These should be
            run with explicit 'tract' constraints in the data query, otherwise
            quanta will be created for jobs with only partial visit coverage.

    step6:
        subset:
        - consolidateDiaSourceTable
        description: >
            Tasks that can be run together, but only after the 'step1', 'step2',
            'step3', and 'step4' subsets

            This step includes visit-level aggregation tasks. Running without
            tract or patch in the data query is recommended, otherwise the
            outputs of consolidateDiaSourceTable will not contain complete visits.

            This subset is separate from step4 to signal to operators to pause
            to assess unexpected image differencing failures before these
            aggregation steps. Otherwise, if run in the same quantum graph,
            aggregated data products (e.g. diaObjects) would not be created if
            one or more of the expected inputs is missing.

\end{verbatim}

\appendix
% Include all the relevant bib files.
% https://lsst-texmf.lsst.io/lsstdoc.html#bibliographies
\section{References} \label{sec:bib}
\renewcommand{\refname}{} % Suppress default Bibliography section
\bibliography{local,lsst,lsst-dm,refs_ads,refs,books}

% Make sure lsst-texmf/bin/generateAcronyms.py is in your path
\section{Acronyms} \label{sec:acronyms}
\input{acronyms.tex}
% If you want glossary uncomment below -- comment out the two lines above
%\printglossaries




\end{document}
