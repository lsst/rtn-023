\documentclass[DM,authoryear,toc]{lsstdoc}
% lsstdoc documentation: https://lsst-texmf.lsst.io/lsstdoc.html
\input{meta}

% Package imports go here.

% Local commands go here.

%If you want glossaries
%\input{aglossary.tex}
%\makeglossaries

\title{Campaign Tooling -- tools for generating, monitoring and tracking data processing campaigns}

% Optional subtitle
% \setDocSubtitle{A subtitle}

\author{%
Brian Yanny, Colin Slater, Sergey Padolski, Yusra AlSayyad, Hsin-Fang Chiang, Huan Lin
}

\setDocRef{RTN-023}
\setDocUpstreamLocation{\url{https://github.com/lsst/rtn-023}}

\date{\vcsDate}

% Optional: name of the document's curator
% \setDocCurator{The Curator of this Document}

\setDocAbstract{%
Tools for working with the Butler to query for status of data collections to generate processing campaigns, for working with BPS, PanDA,  Condor or other workflow systems to monitor campaigns; tools to track success and failure of campaigns, past and present. Requirements described.
}

% Change history defined here.
% Order: oldest first.
% Fields: VERSION, DATE, DESCRIPTION, OWNER NAME.
% See LPM-51 for version number policy.
\setDocChangeRecord{%
  \addtohist{1}{YYYY-MM-DD}{Unreleased.}{Brian Yanny}
}


\begin{document}

% Create the title page.
\maketitle
% Frequently for a technote we do not want a title page  uncomment this to remove the title page and changelog.
% use \mkshorttitle to remove the extra pages

% ADD CONTENT HERE
% You can also use the \input command to include several content files.

\section{Introduction and Definitions}

Data Processing for Rubin occurs at several levels,
as described in https://dmtn-137.lsst.io. 


A 'run' is defined as a run of a BPS-run-spec.yaml through 'bps submit
BPS-run-spec.yaml'.
This run-spec yaml file defines:
\begin{enumerate}

\item An ordered list of pipetasks to be run: 
pipelineYaml: 
"\$OBS\_LSST\_DIR/pipelines/imsim/DRP.yaml\#singleFrame,multiVisit"
These pipetasks may consist of several steps, and these steps may be defined
in 'includes' in the DRP.yaml, for instance.

\item A target cluster to run on, and resource requests such as 
maximum memory (which can be different for different pipelines), number of cpus,
and maximum wallclock time, and re-try attempt limits.

\item A butler configuration yaml

\item an input collection to be extracted from the butler mentioned in
the butler configuration along with a dataQuery which can restict the inputs
to a subset of the full input collection based on 
an SQL string (i.e. just do a few visits or a single tract
or a single band)

\item various other configuration information including location of 
buckets, output path specs, etc

\end{enumerate}

A campaign is defined as one or more related sets of runs, to carry out a
specific data processing goal.  Examples of campaigns might be:

\begin{itemize}

\item (re)process the DC2 dataset, starting with the raw visit exposures,
 into tracts.  

\item Process last night's data (some 1000 visits typically) through
single frame processing pipeline steps.

\item Perform FGCM calibration on all consolidated visits within a given
date range

\item Generate coadds from all 'good' consolidated visits in a given observing
date range that overlap a given set of tracts, use a specific FGCM calibration 

\item Perform Difference Imaging on a specific set of visits, includes gathering
and generation of comparison templates from visits/coadds of date range.


\end{itemize} 

Each run is driven by a yaml-style configuation scripts which are launched
by a command such as 'bps submit'.

It would be necessary, when managing large, complex sets of runs in campaigns, 
such as for nightly or annual Data Release Processing DRP or for 
Prompt/Alert systems, to track all runs in campaigns submitted as well as their 
status with links to logs of individual tasks within the campaigns.  

System performance metrics on campaigns may also find it useful to 
connect directly with 
a campaign tracking system.

The purpose of this document is to describe requirements on such a campaign
tracking and monitoring system.

https://rtn-013.lsst.io

\begin{figure}
\includegraphics[width=\textwidth]{CampaignTooling.jpg}]
\caption{Diagram of Campaign generation (left) and monitoring system (right).
Credit: C. Slater}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{analdrp.png}]
\caption{Diagram of Data Release Processing data flow.
Credit: Y. AlSayyad, H. Chiang}
\end{figure}


\section{Campaign Generation and Tracking Requirements}

 To generate processing campaigns and track them these features are desired:

	\begin{enumerate}

	\item Ability to access up-to-date source of inputs to-be-processed
		into outputs and their current state 
		(already processed, processed and flagged bad, 
		in processing)

	\begin{itemize}
	
	\item May wish to avoid constantly hitting the butler for
	its contents -- so may wish to have another active source
	of inputs and status of processing -- i.e. another database
	which would only occasionally check with the butler to sync up


	\end{itemize}

	\item Ability to generate BPS submit yaml configs 
	(or other top level workflow submit scripts) which request processing
	of subset of inputs needing to be processed, pipelines to be run,
	outputs described.  This could, for instance, be achieved by having
	a set of template yaml configs -- one for each procesing step
	(Single Frame, Consolidate Visit, Photo calib FGCM, 
	astrometric calib skyMap, Coadd/forced Photometry, Difference
	Imaging Analysis) where a 'sed' script or similiar would fill in
	a visit list, a src catalog list, a tract list, etc

	\begin{itemize}

	\item Should be able to 'split' a large set of inputs into
	serveral managable sized pieces.  For instance, DP0.2 consists of
	about 40,000 visits, and a typical Rubin night of observing will
	take 1,000-2,000 science exposures.   So a managable unit for
	single visit processing might be of size 1000 exposures/visits and
	a DP0.2 processing would split the 40,000 visits into 1000 visit
	groups.  Consideration should be given to how long (wallclock time)
	a single campaign 'unit' may be run and to not generally exceed 
	24 hours to a few days to avoid interruptions from downtimes.
	Also depends on ability of system to recover from interruptions and
	continue without needing to restart the whole campaign from
	the beginning.
	

	\item  Should allow(?) 'last minute editing' of configs before
	submission to tweak something, change a pipeline version or
	visit input list

	\item each campaign should be identified by some unique id string, 
	perhaps same as workflow manager identifying string?

	\end{itemize}

	\item Ability to store and manage a set of BPS submit scripts 
	(or equivalent).
	For instance, as each BPS submit yaml is performed, the yaml
	script contents could be saved in a database with metadata such
	as time of submission, location of physical processing
	(SLAC, IN2P3, UK, Cloud, user-laptop), current state (non started,
	running, finished-no-errors, finished-with-errors)


	\item Ability to quickly access and view logs of each 
	campaign's individual tasks/jobs to determine errors

	\item Ability to view overall state of running and recently
	completed campaigns, without being overwhelmed by long-ago completed
	or abandoned campaigns.

	\item Ability to straight-forwardly dump dataids (or filenames
	with paths) and metadata (exposureid, filter) of inputs or outputs
	from some specific job or task of the processing campaign to
	track down issues and errors.  This ability may be present already
	but some 'helper' scripts could simplify the process.

	\item Ability to watch, in realtime, as jobs or tasks within
	a campaign or set of campaigns are running.  There is a tool	
	called 'bps report' which does this at some level, showing how
	many individual inputs in a campaign set have been processed
	so far.

	\end {enumerate}

\begin{figure}
\includegraphics[width=\textwidth]{bpsreport.jpg}]
\caption{Sample BPS report display showing number of quanta for each task within a give run.
Credit: M. Gower}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{pandadash0.jpg}]
\caption{Top level PanDA monitoring dashboard of running and recent jobs
Credit: S. Padolski}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{tasks1.jpg}]
\caption{Next level task list for first job in list
Credit: H. Lin}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{tasks4.jpg}]
\caption{Next level status for isr task (2nd from bottom in prev list).
Credit: H. Lin}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{tasks5.jpg}]
\caption{Next level status for some of the 603 quanta in this task.
Credit: H. Lin}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{log0.jpg}]
\caption{Getting to the stdout/stderr log for this quantum. 
Credit: H. Lin}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{log1.jpg}]
\caption{A look at stdout/stderr -- if there was an error, one can view it here to help diagnose an issue.
Credit: H. Lin}
\end{figure}

\appendix
% Include all the relevant bib files.
% https://lsst-texmf.lsst.io/lsstdoc.html#bibliographies
\section{References} \label{sec:bib}
\renewcommand{\refname}{} % Suppress default Bibliography section
\bibliography{local,lsst,lsst-dm,refs_ads,refs,books}

% Make sure lsst-texmf/bin/generateAcronyms.py is in your path
\section{Acronyms} \label{sec:acronyms}
\input{acronyms.tex}
% If you want glossary uncomment below -- comment out the two lines above
%\printglossaries




\end{document}
